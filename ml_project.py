# -*- coding: utf-8 -*-
"""ML_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oIOQMn7hO7jkKUoOSiWQoTHaK-bGG4qW
"""

# from google.colab import drive
# drive.mount('/content/drive')

import os
import time
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
import gc
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import f1_score
from collections import Counter
import torch.nn.functional as F
from torch.optim.lr_scheduler import StepLR
import noisereduce as nr
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import seaborn as sns

# 1. Recursively collect .wav files from all subdirectories
def get_wav_files(wav_dir):
    """Recursively collect all .wav files in the directory and its subdirectories."""
    wav_files = []
    for root, dirs, files in os.walk(wav_dir):
        for file in files:
            if file.endswith('.wav'):
                wav_files.append(os.path.join(root, file))

    print(f"Total .wav files found: {len(wav_files)}")
    return wav_files

def apply_noise_reduction(y, sr=16000):
    """Применение шумоподавления к аудиоданным."""
    reduced_noise_audio = nr.reduce_noise(y=y, sr=sr)
    return reduced_noise_audio

def load_and_clean_audio(file_path, sr=16000, trim_silence=True, cut_length=None):
    try:
        y, sr = librosa.load(file_path, sr=sr)
        print(f"Loaded {file_path}, Audio Length: {len(y)}")

        y = apply_noise_reduction(y, sr)
        print(f"Reduced noise {file_path}, Audio Length after reducing noise: {len(y)}")
        
        y = y / np.max(np.abs(y))

        if trim_silence:
            y, _ = librosa.effects.trim(y, top_db=20)
            print(f"Trimmed silence from {file_path}, Audio Length after trimming: {len(y)}")

        if cut_length:
            y = y[:int(sr * cut_length)]
            print(f"Cut {file_path} to {cut_length} seconds, Final Length: {len(y)}")

        return y, sr
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return None, None

def generate_spectrogram(y, sr=16000, n_mels=64):
    """Generate a mel-spectrogram from the audio data."""
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    S_dB = librosa.power_to_db(S, ref=np.max)
    return S_dB

def save_spectrogram_to_npy(spectrogram, original_wav_file, save_dir):
    """Save the spectrogram to a .npy file."""
    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(original_wav_file).replace('.wav', '.npy')  # Replace .wav with .npy
    save_path = os.path.join(save_dir, file_name)
    np.save(save_path, spectrogram)
    print(f"Spectrogram saved to {save_path}")

    time.sleep(1)

def process_audio_to_spectrograms_and_save_in_chunks(wav_dir, sr=16000, n_mels=64, cut_length=None, save_dir='/path/to/save/spectrograms'):
    """Process audio files, generate spectrograms, and save them as .npy files one at a time to avoid memory overload."""
    file_paths = get_wav_files(wav_dir)

    for i, file in enumerate(file_paths):
        npy_file_path = os.path.join(save_dir, os.path.basename(file).replace('.wav', '.npy'))
        print(f"Checking if {npy_file_path} exists...")

        if os.path.exists(npy_file_path):
            print(f"Skipping {file} (already processed)")
            continue

        try:
            y, sr = load_and_clean_audio(file, sr=sr, cut_length=cut_length)
            if y is None:
                continue

            spectrogram = generate_spectrogram(y, sr, n_mels)

            save_spectrogram_to_npy(spectrogram, file, save_dir)

            del y, spectrogram
            gc.collect()

        except Exception as e:
            print(f"Error processing or saving spectrogram for {file}: {e}")

        if i % 10 == 0:
            print(f"Progress: Processed {i + 1}/{len(file_paths)} files.")

    print("All spectrograms processed and saved.")

# 2. Preliminary analysis
# Spectogram mean and variance
def calculate_spectrogram_stats(spectrograms):
    mean_list = []
    var_list = []
    
    for spec in spectrograms:
        mean_list.append(np.mean(spec))
        var_list.append(np.var(spec))
        
    print(f"Average Spectrogram Mean: {np.mean(mean_list):.4f}")
    print(f"Average Spectrogram Variance: {np.mean(var_list):.4f}")

#Tracking Most and Least Difficult Samples
def track_difficult_samples(model, loader, device):
    model.eval()
    sample_difficulties = []
    
    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = F.cross_entropy(outputs, y_batch, reduction='none')
            for i, l in enumerate(loss.cpu().numpy()):
                sample_difficulties.append((X_batch[i], l, y_batch[i].cpu().numpy()))

    # Sort samples by loss
    sample_difficulties = sorted(sample_difficulties, key=lambda x: x[1], reverse=True)

    most_difficult = sample_difficulties[:5]
    least_difficult = sample_difficulties[-5:]
    
    print("Most difficult samples:")
    for sample in most_difficult:
        print(f"Sample Loss: {sample[1]:.4f}")
    
    print("Least difficult samples:")
    for sample in least_difficult:
        print(f"Sample Loss: {sample[1]:.4f}")

# Finding Similar Samples Based on Activations
def perform_clustering_and_plot(model, loader, device, n_clusters=5):
    model.eval()
    activations = []
    labels = []

    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch = X_batch.to(device)
            
            # Forward pass through the network up to the first fully connected layer (fc1)
            model.eval() 
            x = model.pool(F.relu(model.conv1(X_batch)))  # Apply conv1 and pooling
            x = model.pool(F.relu(model.conv2(x)))  # Apply conv2 and pooling

            # Flatten before fully connected layer
            x = x.view(-1, model.fc1.in_features)

            activations.append(model.fc1(x).cpu().numpy())  # Collect activations from fc1
            labels.extend(y_batch.cpu().numpy())

    activations = np.vstack(activations)

    # Dimensionality reduction using PCA 
    pca = PCA(n_components=2)
    reduced_activations = pca.fit_transform(activations)

    # Clustering using KMeans
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(reduced_activations)

    # Plotting 
    sns.scatterplot(x=reduced_activations[:, 0], y=reduced_activations[:, 1], hue=clusters, palette="deep")
    plt.title(f"Clustering activations at Epoch")
    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")
    plt.show()



# 3. Visualize a Few npy's <-> in main the mount is set to "3"
def visualize_spectrograms(npy_files, num_to_visualize=3):

    for i, npy_file in enumerate(npy_files[:num_to_visualize]):
        try:
            spectrogram = np.load(npy_file)
            plt.figure(figsize=(10, 4))
            librosa.display.specshow(spectrogram, sr=16000, x_axis='time', y_axis='mel')
            plt.colorbar(format='%+2.0f dB')
            plt.title(f"Spectrogram of {os.path.basename(npy_file)}")
            plt.tight_layout()
            plt.show()
        except Exception as e:
            print(f"Error loading or visualizing {npy_file}: {e}")

def load_npy_files(npy_dir):
    """Recursively load all .npy files from the directory."""
    npy_files = []
    for root, dirs, files in os.walk(npy_dir):
        for file in files:
            if file.endswith('.npy'):
                npy_files.append(os.path.join(root, file))

    print(f"Total .npy files found: {len(npy_files)}")
    return npy_files

def load_spectrogram_from_npy(npy_file):
    """Load a spectrogram from a .npy file."""
    try:
        spectrogram = np.load(npy_file)
        return spectrogram
    except Exception as e:
        print(f"Error loading {npy_file}: {e}")
        return None

def assign_labels_from_npy(files):
    """Assign class labels based on the speaker IDs in the .npy filenames."""
    class_1_speakers = ['F1', 'F7', 'F8', 'M3', 'M6', 'M8']  # Class 1 speakers
    labels = []

    for file in files:
        speaker_id = os.path.basename(file).split('_')[0].upper()
        if speaker_id in class_1_speakers:
            labels.append(1)  # Class 1
        else:
            labels.append(0)  # Class 0

    return labels

class SpectrogramDataset(Dataset):
    def __init__(self, spectrograms, labels):
        self.spectrograms = spectrograms
        self.labels = labels

    def __len__(self):
        return len(self.spectrograms)

    def __getitem__(self, idx):
        spectrogram = torch.tensor(self.spectrograms[idx], dtype=torch.float32).unsqueeze(0)
        if self.labels is not None:
            label = torch.tensor(self.labels[idx], dtype=torch.long)
            return spectrogram, label
        else:
            return spectrogram

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)

        self.fc1 = None
        self.fc2 = nn.Linear(128, 2)
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))

        if self.fc1 is None:
          flattened_size = x.view(x.size(0), -1).size(1)
          self.fc1 = nn.Linear(flattened_size, 128)
          self.fc1 = self.fc1.to(x.device)

        x = x.view(-1, self.fc1.in_features)

        x = F.relu(self.fc1(x))
        x = self.fc2(x)

        return x

# MLP (Multi-Layer Perceptron)
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = None
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 2)

    def forward(self, x):
      if self.fc1 is None:
        # плющим данные//flattening data
        flattened_size = x.view(x.size(0), -1).size(1)
        self.fc1 = nn.Linear(flattened_size, 128)
        self.fc1 = self.fc1.to(x.device)  # to device (CPU or GPU)

      x = x.view(-1, self.fc1.in_features)
      x = F.relu(self.fc1(x))
      x = F.relu(self.fc2(x))
      x = self.fc3(x)

      return x

def compare_models(train_loader, val_loader, labels):
    models = {
        'SimpleCNN': SimpleCNN(),
        # other models - MLP or Random Forest
    }

    results = {}
    best_f1 = 0.0  # Инициализируем с 0
    best_model = None

    # Обучение каждой модели и вычисление F1-оценки
    for model_name, model in models.items():
        print(f"\n=== Обучение и оценка: {model_name} ===")
        f1 = train_and_evaluate(model, train_loader, val_loader, labels, num_epochs=10, lr=0.001)

        # Проверим, что f1 не None или NaN
        if f1 is not None and not np.isnan(f1):
            results[model_name] = f1
            print(f"{model_name}: F1-оценка = {f1:.4f}")

            # Сравниваем F1-оценки, чтобы найти лучшую модель
            if f1 > best_f1:
                best_f1 = f1
                best_model = model
        else:
            print(f"F1-оценка для {model_name} не была рассчитана корректно.")

    print("\n=== Сравнение моделей ===")
    for model_name, f1 in results.items():
        print(f"{model_name}: F1-оценка = {f1:.4f}")

    # Проверим, что есть лучшая модель
    if best_model is None:
        raise ValueError("Ни одна из моделей не была обучена корректно или не имеет валидных F1-оценок.")

    # Возвращаем модель с лучшей F1-оценкой
    return best_model

def calculate_class_weights(labels):
    """Calculate class weights based on the labels."""
    class_counts = Counter(labels)
    total_samples = sum(class_counts.values())
    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}
    return class_weights

def train_and_evaluate(model, train_loader, val_loader, labels, num_epochs=10, lr=0.001, n_clusters=5):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    #depending on labels findin weights
    class_weights = calculate_class_weights(labels)
    weights = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float32).to(device)

    # initialise optimizer (Adam here, we can add others), scheduler, we can change criterion

    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)
    criterion = nn.CrossEntropyLoss(weight=weights)

    # train model
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        scheduler.step()

        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}')

        print(f"Clustering activations after Epoch {epoch+1}")
        perform_clustering_and_plot(model, val_loader, device, n_clusters=n_clusters)

    # Estimation on val
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(y_batch.cpu().numpy())

    # Вычисление F1-score
    f1 = f1_score(all_labels, all_preds, average='macro')
    return f1

def calculate_f1_score(model, loader, device):
    model.eval()
    all_labels = []
    all_predictions = []

    with torch.no_grad():
        for spectrograms, labels in loader:
            spectrograms, labels = spectrograms.to(device), labels.to(device)
            outputs = model(spectrograms)
            _, predicted = torch.max(outputs, 1)
            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

    f1 = f1_score(all_labels, all_predictions, average='macro')
    return f1

"""Saving for pytorch models - CNN,.."""

def save_model_neural(model, path):
    torch.save(model, path)



"""Saving for classical ML models - Random Forrest"""

import joblib

def save_model_classic(model, path):
    joblib.dump(model, path)
    print(f"Model saved to {path}")

def load_model_classic(path):
    model = joblib.load(path)
    print(f"Model loaded from {path}")
    return model

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

"""Best parameters: {'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': 20, 'bootstrap': False}
RandomForest F1-Score with best parameters: 0.9476

Best parameters: {'n_estimators': 25, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 15, 'criterion': 'entropy', 'bootstrap': False}
RandomForest F1-Score with best parameters: 0.9598

Fitting 3 folds for each of 10 candidates, totalling 30 fits
Best parameters: {'n_estimators': 25, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_depth': None, 'criterion': 'entropy', 'bootstrap': False}
RandomForest F1-Score with best parameters: 0.9598


Best parameters: {'n_estimators': 25, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_depth': 12, 'criterion': 'gini', 'bootstrap': False}
accuracy score:  0.9733333333333334
RandomForest F1-Score with best parameters: 0.9638

❌  Parameters currently in use:

{'bootstrap': True,
 'criterion': 'mse',
 'max_depth': None,
 'max_features': 'auto',
 'max_leaf_nodes': None,
 'min_impurity_decrease': 0.0,
 'min_impurity_split': None,
 'min_samples_leaf': 1,
 'min_samples_split': 2,
 'min_weight_fraction_leaf': 0.0,
 'n_estimators': 10,
 'n_jobs': 1,
 'oob_score': False,
 'random_state': 42,
 'verbose': 0,
 'warm_start': False}

 accuracy dropped to 0.82 with the oob_score + max_leaf_nodes => we'll leave default values for them
"""

def random_forest_with_random_search(spectrograms, labels, test_size=0.2, n_iter=10, cv=3):
    # spectograms -> vectors (flattening)
    X = np.array([s.flatten() for s in spectrograms])
    y = np.array(labels)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Standartizing
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)



    param_distributions = {
        'criterion': ['gini', 'entropy'],
        'n_estimators': range(15,35, 5),
        'max_depth': [None, 3, 6, 9, 12, 15],
        'min_samples_split': [9,10],
        'min_samples_leaf': [1, 2, 3],
        'bootstrap': [True, False]
        # things below dropped the f1 and accuracy
        # 'max_leaf_nodes': range(1,10, 2),
        # 'oob_score': [True, False]
    }

    rf = RandomForestClassifier(random_state=42)

    random_search = RandomizedSearchCV(
        estimator=rf,
        param_distributions=param_distributions,
        n_iter=n_iter,
        cv=cv,
        verbose=2,
        random_state=42,
        n_jobs=-1
    )

    random_search.fit(X_train, y_train)

    best_params = random_search.best_params_
    print(f"Лучшие параметры: {best_params}")

    best_rf = random_search.best_estimator_
    y_pred = best_rf.predict(X_test)
    f1 = f1_score(y_test, y_pred, average='macro')

    print(f"accuracy score:  {accuracy_score(y_test, y_pred)}")


    print(f"RandomForest F1-Score with best params: {f1:.4f}")

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Negative", "Positive"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix for RandomForest')
    plt.show()

    return best_rf, best_params, f1

if __name__ == "__main__":
    wav_dir = '/Users/sunsosun/Desktop/ML_DEPLOY/daps'
    save_dir = '/Users/sunsosun/Desktop/ML_DEPLOY/npy_spectrograms'
    cut_length = 10
    n_mels = 64  # Number of Mel bands

    #generate_spectrogram(y, sr, n_mels)
    process_audio_to_spectrograms_and_save_in_chunks(wav_dir, sr=16000, n_mels=n_mels, cut_length=cut_length, save_dir=save_dir)

    npy_files = load_npy_files(save_dir)
    visualize_spectrograms(npy_files, num_to_visualize=3)
    spectrograms = [load_spectrogram_from_npy(file) for file in npy_files if load_spectrogram_from_npy(file) is not None]
    labels = assign_labels_from_npy(npy_files)

    train_val_spectrograms, test_spectrograms, train_val_labels, test_labels = train_test_split(spectrograms, labels, test_size=0.2, random_state=42, stratify=labels)
    train_spectrograms, val_spectrograms, train_labels, val_labels = train_test_split(train_val_spectrograms, train_val_labels, test_size=0.125, random_state=42, stratify=train_val_labels)
    # 0.125 * 80% = 10%

    train_dataset = SpectrogramDataset(train_spectrograms, train_labels)
    val_dataset = SpectrogramDataset(val_spectrograms, val_labels)
    test_dataset = SpectrogramDataset(test_spectrograms, test_labels)

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    best_model = compare_models(train_loader, val_loader, labels)

    model_save_path = '/Users/sunsosun/Desktop/ML_DEPLOY/best_model.pth'
    save_model_neural(best_model, model_save_path)
    print(f"Model saved to: {model_save_path}")

    f1_val = calculate_f1_score(best_model, val_loader, device)
    print(f"f1-score on validation_set (10%): {f1_val:.4f}")

    f1_test = calculate_f1_score(best_model, test_loader, device)
    print(f"f1-score on test_set (20%): {f1_test:.4f}")

    calculate_spectrogram_stats(spectrograms)
    print("Tracking most and least difficult samples on validation set:")
    track_difficult_samples(best_model, val_loader, device)


    #I'll add Random Forest just because i like it
    # best_rf_model, best_params, rf_f1_score = random_forest_with_random_search(spectrograms, labels, test_size=0.2, n_iter=10, cv=3)

    # save_model_classic(best_rf_model, 'best_random_forest_model.joblib')

