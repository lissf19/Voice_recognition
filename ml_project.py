# -*- coding: utf-8 -*-
"""ML_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oIOQMn7hO7jkKUoOSiWQoTHaK-bGG4qW
"""

# from google.colab import drive
# drive.mount('/content/drive')

import os
import time
import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
import gc
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import f1_score
from collections import Counter
import torch.nn.functional as F
from torch.optim.lr_scheduler import StepLR
import noisereduce as nr
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns


def get_wav_files(wav_dir):
    """Recursively collect all .wav files in the directory and its subdirectories."""
    wav_files = []
    for root, dirs, files in os.walk(wav_dir):
        for file in files:
            if file.endswith('.wav'):
                wav_files.append(os.path.join(root, file))

    print(f"Total .wav files found: {len(wav_files)}")
    return wav_files

def apply_noise_reduction(y, sr=16000):
    reduced_noise_audio = nr.reduce_noise(y=y, sr=sr)
    return reduced_noise_audio

def load_audio(file_path,sr=16000):
    try:
      y, sr = librosa.load(file_path, sr=sr)
      print(f"Loaded {file_path}, Audio Length: {len(y)}")
      return y, sr
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return None, None

def clean_audio(y,sr,file_path, trim_silence=True, cut_length=None):
    try:
        y = y / np.max(np.abs(y))

        if trim_silence:
            y, _ = librosa.effects.trim(y, top_db=20)
            print(f"Trimmed silence from {file_path}, Audio Length after trimming: {len(y)}")

        if cut_length:
            y = y[:int(sr * cut_length)]
            print(f"Cut {file_path} to {cut_length} seconds, Final Length: {len(y)}")

        return y, sr
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return None, None

def generate_spectrogram(y, sr=16000, n_mels=64):
    """Generate a mel-spectrogram from the audio data."""
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    S_dB = librosa.power_to_db(S, ref=np.max)
    return S_dB

def save_spectrogram_to_npy(spectrogram, original_wav_file, save_dir, processed=False):
    """Save the spectrogram to a .npy file."""
    os.makedirs(save_dir, exist_ok=True)

    file_name = os.path.basename(original_wav_file).replace('.wav', '_processed.npy') if processed else os.path.basename(original_wav_file).replace('.wav', '.npy')  # Replace .wav with .npy
    save_path = os.path.join(save_dir, file_name)
    np.save(save_path, spectrogram)
    print(f"Spectrogram saved to {save_path}")

def process_audio_to_spectrograms_and_save_in_chunks(wav_dir, sr=16000, n_mels=64, cut_length=None, save_dir='/path/to/save/spectrograms'):
    """Process audio files, generate spectrograms, and save them as .npy files one at a time to avoid memory overload."""
    file_paths = get_wav_files(wav_dir)

    for i, file in enumerate(file_paths):
        npy_file_path = os.path.join(save_dir, os.path.basename(file).replace('.wav', '.npy'))
        print(f"Checking if {npy_file_path} exists...")

        if os.path.exists(npy_file_path):
            print(f"Skipping {file} (already processed)")
            continue

        try:
            y, sr = load_audio(file, sr=sr)
            if y is None:
                continue

            spectrogram = generate_spectrogram(y, sr, n_mels)
            save_spectrogram_to_npy(spectrogram, file, save_dir)

            y, sr = clean_audio(y,sr,file,cut_length=cut_length)
            spectrogram = generate_spectrogram(y, sr, n_mels)
            save_spectrogram_to_npy(spectrogram, file, save_dir, processed=True)

            del y, spectrogram
            gc.collect()

        except Exception as e:
            print(f"Error processing or saving spectrogram for {file}: {e}")

        if i % 10 == 0:
            print(f"Progress: Processed {i + 1}/{len(file_paths)} files.")

    print("All spectrograms processed and saved.")

# 2. Preliminary analysis
# Spectogram mean and variance
# Distributions of means and variances
def calculate_spectrogram_stats(spectrograms):
    mean_list = []
    var_list = []
    
    for spec in spectrograms:
        mean_list.append(np.mean(spec))
        var_list.append(np.var(spec))
        
    print(f"Average Spectrogram Mean: {np.mean(mean_list):.4f}")
    print(f"Average Spectrogram Variance: {np.mean(var_list):.4f}")

    plt.figure(figsize=(6, 8))

    plt.subplot(2, 1, 1)
    plt.hist(mean_list, bins=20, color='skyblue', edgecolor='black')
    plt.title("Distribution of Spectrogram Means")
    plt.xlabel("Mean Intensity")
    plt.ylabel("Frequency")

    # Plot histogram of variances
    plt.subplot(2, 1, 2)
    plt.hist(var_list, bins=20, color='yellow', edgecolor='black')
    plt.title("Distribution of Spectrogram Variances")
    plt.xlabel("Variance in Intensity")
    plt.ylabel("Frequency")
    
    plt.tight_layout()
    plt.show()

def plot_confusion_matrix(model, loader, device, class_names=None):
    model.eval()
    all_labels = []
    all_predictions = []

    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            all_labels.extend(y_batch.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

    # Generate the confusion matrix
    cm = confusion_matrix(all_labels, all_predictions)
    
    # Display the confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    fig, ax = plt.subplots(figsize=(8, 8))
    disp.plot(cmap=plt.cm.Blues, ax=ax)

    for texts in ax.texts:
        texts.set_visible(False)
    
    label_texts = [
        f'TN\n{cm[0, 0]}',  
        f'FP\n{cm[0, 1]}',  
        f'FN\n{cm[1, 0]}', 
        f'TP\n{cm[1, 1]}'  
    ]

    # Positions in the confusion matrix for TN, FP, FN, TP
    positions = [(0, 0), (0, 1), (1, 0), (1, 1)]
    
    for pos, label in zip(positions, label_texts):
        ax.text(pos[1], pos[0], label, ha='center', va='center', color='white' if cm[pos[0], pos[1]] > cm.max() / 2 else 'black', fontsize=12, fontweight='bold')

    plt.title("Confusion Matrix with TN, FP, FN, TP Labels")
    plt.show()

#Tracking Most and Least Difficult Samples
def track_difficult_samples(model, loader, device, file_ids=None):
    model.eval()
    sample_difficulties = []
    
    with torch.no_grad():
        for i, (X_batch, y_batch) in enumerate(loader):
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = F.cross_entropy(outputs, y_batch, reduction='none')

            batch_file_ids = file_ids[i * loader.batch_size: (i + 1) * loader.batch_size] if file_ids else [None] * len(y_batch)
            
            for j, l in enumerate(loss.cpu().numpy()):
                sample_difficulties.append((batch_file_ids[j], l, y_batch[j].cpu().numpy()))

    # Sort samples by loss
    sample_difficulties = sorted(sample_difficulties, key=lambda x: x[1], reverse=True)

    most_difficult = sample_difficulties[:5]
    least_difficult = sample_difficulties[-5:]
    
    print("Most difficult samples:")
    for sample in most_difficult:
        print(f"Sample ID: {sample[0]}, Sample Class: {sample[2]}, Loss: {sample[1]:.4f}")
    
    print("Least difficult samples:")
    for sample in least_difficult:
        print(f"Sample ID: {sample[0]}, Sample Class: {sample[2]}, Loss: {sample[1]:.4f}")

# Finding Similar Samples Based on Activations
def perform_clustering_and_plot(model, loader, device, n_clusters=5):
    model.eval()
    activations = []
    labels = []

    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch = X_batch.to(device)
            
            # Forward pass through the network up to the first fully connected layer (fc1)
            model.eval() 
            x = model.pool(F.relu(model.conv1(X_batch)))  # Apply conv1 and pooling
            x = model.pool(F.relu(model.conv2(x)))  # Apply conv2 and pooling

            # Flatten before fully connected layer
            x = x.view(-1, model.fc1.in_features)

            activations.append(model.fc1(x).cpu().numpy())  # Collect activations from fc1
            labels.extend(y_batch.cpu().numpy())

    activations = np.vstack(activations)

    # Dimensionality reduction using PCA 
    pca = PCA(n_components=2)
    reduced_activations = pca.fit_transform(activations)

    # Clustering using KMeans
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(reduced_activations)

    # Plotting 
    sns.scatterplot(x=reduced_activations[:, 0], y=reduced_activations[:, 1], hue=clusters, palette="deep")
    plt.title(f"Clustering activations at Epoch")
    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")
    plt.show()



# 3. Visualize a Few npy's <-> in main the mount is set to "5"
def visualize_spectrograms(npy_files, num_to_visualize=3):

    for i, npy_file in enumerate(npy_files[:num_to_visualize]):
        try:
            spectrogram = np.load(npy_file)
            plt.figure(figsize=(10, 4))
            librosa.display.specshow(spectrogram, sr=16000, x_axis='time', y_axis='mel')
            plt.colorbar(format='%+2.0f dB')
            plt.title(f"Spectrogram of {os.path.basename(npy_file)}")
            plt.tight_layout()
            plt.show()
        except Exception as e:
            print(f"Error loading or visualizing {npy_file}: {e}")

def load_npy_files(npy_dir):
    """Recursively load all .npy files from the directory."""
    npy_files = []
    processed_npy_files = []
    for root, dirs, files in os.walk(npy_dir):
        for file in files:
            if file.endswith('processed.npy'):
                processed_npy_files.append(os.path.join(root, file))
            elif file.endswith('.npy'):
                npy_files.append(os.path.join(root, file))

    print(f"Total .npy files found: {len(npy_files)}")
    return npy_files, processed_npy_files

def load_spectrogram_from_npy(npy_file):
    """Load a spectrogram from a .npy file."""
    try:
        spectrogram = np.load(npy_file)
        return spectrogram
    except Exception as e:
        print(f"Error loading {npy_file}: {e}")
        return None

def assign_labels_from_npy(files):
    """Assign class labels based on the speaker IDs in the .npy filenames."""
    class_1_speakers = ['F1', 'F7', 'F8', 'M3', 'M6', 'M8']  # Class 1 speakers
    labels = []

    for file in files:
        speaker_id = os.path.basename(file).split('_')[0].upper()
        if speaker_id in class_1_speakers:
            labels.append(1)  # Class 1
        else:
            labels.append(0)  # Class 0

    return labels

class SpectrogramDataset(Dataset):
    def __init__(self, spectrograms, labels):
        self.spectrograms = spectrograms
        self.labels = labels

    def __len__(self):
        return len(self.spectrograms)

    def __getitem__(self, idx):
        spectrogram = torch.tensor(self.spectrograms[idx], dtype=torch.float32).unsqueeze(0)
        if self.labels is not None:
            label = torch.tensor(self.labels[idx], dtype=torch.long)
            return spectrogram, label
        else:
            return spectrogram

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)

        self.fc1 = None
        self.fc2 = nn.Linear(128, 2)
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))

        if self.fc1 is None:
          flattened_size = x.view(x.size(0), -1).size(1)
          self.fc1 = nn.Linear(flattened_size, 128)
          self.fc1 = self.fc1.to(x.device)

        x = x.view(-1, self.fc1.in_features)

        x = F.relu(self.fc1(x))
        x = self.fc2(x)

        return x

# MLP (Multi-Layer Perceptron)
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = None
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 2)

    def forward(self, x):
      if self.fc1 is None:
        # flattening data
        flattened_size = x.view(x.size(0), -1).size(1)
        self.fc1 = nn.Linear(flattened_size, 128)
        self.fc1 = self.fc1.to(x.device)  # to device (CPU or GPU)

      x = x.view(-1, self.fc1.in_features)
      x = F.relu(self.fc1(x))
      x = F.relu(self.fc2(x))
      x = self.fc3(x)

      return x

def compare_models(train_loader, val_loader, labels):
    models = {
        'SimpleCNN': SimpleCNN(),
        # other models - MLP or Random Forest
    }

    results = {}
    best_f1 = 0.0  
    best_model = None

    # Training + f1
    for model_name, model in models.items():
        print(f"\n=== Training and estimation: {model_name} ===")
        f1 = train_and_evaluate(model, train_loader, val_loader, labels, num_epochs=10, lr=0.001)

     
        if f1 is not None and not np.isnan(f1):
            results[model_name] = f1
            print(f"{model_name}: F1-оценка = {f1:.4f}")

            # Comparing f1's to take the best model - in the base case we have only CNN (MLP+RF ready)
            if f1 > best_f1:
                best_f1 = f1
                best_model = model
        else:
            print(f"F1-score for {model_name} wasn't calculated correctly")

    print("\n=== Comparing models ===")
    for model_name, f1 in results.items():
        print(f"{model_name}: F1-оценка = {f1:.4f}")

   
    if best_model is None:
        raise ValueError("No valid models with valid F1-scores.")

    # Best f1 model
    return best_model

def calculate_class_weights(labels):
    """Calculate class weights based on the labels."""
    class_counts = Counter(labels)
    total_samples = sum(class_counts.values())
    class_weights = {cls: total_samples / count for cls, count in class_counts.items()}
    return class_weights

def train_and_evaluate(model, train_loader, val_loader, labels, num_epochs=10, lr=0.001, n_clusters=5):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    #depending on labels findin weights
    class_weights = calculate_class_weights(labels)
    weights = torch.tensor([class_weights[0], class_weights[1]], dtype=torch.float32).to(device)

    # initialise optimizer (Adam here, we can add others), scheduler, we can change criterion

    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)
    criterion = nn.CrossEntropyLoss(weight=weights)

    # train model
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        scheduler.step()

        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}')

        print(f"Clustering activations after Epoch {epoch+1}")
        perform_clustering_and_plot(model, val_loader, device, n_clusters=n_clusters)

    # Estimation on val
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(y_batch.cpu().numpy())

    # F1-score estimation
    f1 = f1_score(all_labels, all_preds, average='macro')
    return f1

def calculate_f1_score(model, loader, device):
    model.eval()
    all_labels = []
    all_predictions = []

    with torch.no_grad():
        for spectrograms, labels in loader:
            spectrograms, labels = spectrograms.to(device), labels.to(device)
            outputs = model(spectrograms)
            _, predicted = torch.max(outputs, 1)
            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

    f1 = f1_score(all_labels, all_predictions, average='macro')
    return f1

"""Saving for pytorch models - CNN,.."""

def save_model_neural(model, path):
    torch.save(model, path)



"""Saving for classical ML models - Random Forrest"""

import joblib

def save_model_classic(model, path):
    joblib.dump(model, path)
    print(f"Model saved to {path}")

def load_model_classic(path):
    model = joblib.load(path)
    print(f"Model loaded from {path}")
    return model

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

"""Best parameters: {'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': 20, 'bootstrap': False}
RandomForest F1-Score with best parameters: 0.9476

Best parameters: {'n_estimators': 25, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 15, 'criterion': 'entropy', 'bootstrap': False}
RandomForest F1-Score with best parameters: 0.9598

Fitting 3 folds for each of 10 candidates, totalling 30 fits
Best parameters: {'n_estimators': 25, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_depth': None, 'criterion': 'entropy', 'bootstrap': False}
RandomForest F1-Score with best parameters: 0.9598


Best parameters: {'n_estimators': 25, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_depth': 12, 'criterion': 'gini', 'bootstrap': False}
accuracy score:  0.9733333333333334
RandomForest F1-Score with best parameters: 0.9638

❌  Parameters currently in use:

{'bootstrap': True,
 'criterion': 'mse',
 'max_depth': None,
 'max_features': 'auto',
 'max_leaf_nodes': None,
 'min_impurity_decrease': 0.0,
 'min_impurity_split': None,
 'min_samples_leaf': 1,
 'min_samples_split': 2,
 'min_weight_fraction_leaf': 0.0,
 'n_estimators': 10,
 'n_jobs': 1,
 'oob_score': False,
 'random_state': 42,
 'verbose': 0,
 'warm_start': False}

 accuracy dropped to 0.82 with the oob_score + max_leaf_nodes => we'll leave default values for them
"""

def random_forest_with_random_search(spectrograms, labels, test_size=0.2, n_iter=10, cv=3):
    # spectograms -> vectors (flattening)
    X = np.array([s.flatten() for s in spectrograms])
    y = np.array(labels)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Standartizing
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)



    param_distributions = {
        'criterion': ['gini', 'entropy'],
        'n_estimators': range(15,35, 5),
        'max_depth': [None, 3, 6, 9, 12, 15],
        'min_samples_split': [9,10],
        'min_samples_leaf': [1, 2, 3],
        'bootstrap': [True, False]
        # things below dropped the f1 and accuracy
        # 'max_leaf_nodes': range(1,10, 2),
        # 'oob_score': [True, False]
    }

    rf = RandomForestClassifier(random_state=42)

    random_search = RandomizedSearchCV(
        estimator=rf,
        param_distributions=param_distributions,
        n_iter=n_iter,
        cv=cv,
        verbose=2,
        random_state=42,
        n_jobs=-1
    )

    random_search.fit(X_train, y_train)

    best_params = random_search.best_params_
    print(f"Лучшие параметры: {best_params}")

    best_rf = random_search.best_estimator_
    y_pred = best_rf.predict(X_test)
    f1 = f1_score(y_test, y_pred, average='macro')

    print(f"accuracy score:  {accuracy_score(y_test, y_pred)}")


    print(f"RandomForest F1-Score with best params: {f1:.4f}")

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Negative", "Positive"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix for RandomForest')
    plt.show()

    return best_rf, best_params, f1

if __name__ == "__main__":
    wav_dir = '/Users/sunsosun/Desktop/ML_DEPLOY/daps'
    save_dir = '/Users/sunsosun/Desktop/ML_DEPLOY/npy_spectrograms'
    cut_length = 10
    n_mels = 64  # Number of Mel bands

    process_audio_to_spectrograms_and_save_in_chunks(wav_dir, sr=16000, n_mels=n_mels, cut_length=cut_length, save_dir=save_dir)

    npy_files, processed_npy_files = load_npy_files(save_dir)

    print(f"Raw .npy files Spectrograms:")
    visualize_spectrograms(npy_files, num_to_visualize=5)

    print(f"Processed .npy files Spectrograms:")
    visualize_spectrograms(processed_npy_files, num_to_visualize=5)

    processed_spectrograms = [load_spectrogram_from_npy(file) for file in processed_npy_files if load_spectrogram_from_npy(file) is not None]
    raw_data_spectrograms = [load_spectrogram_from_npy(file) for file in npy_files if load_spectrogram_from_npy(file) is not None]
    
    labels = assign_labels_from_npy(processed_npy_files)

    print("Statistics for Processed Spectrograms:")
    calculate_spectrogram_stats(processed_spectrograms)
    print("Statistics for Raw Spectrograms:")
    calculate_spectrogram_stats(raw_data_spectrograms)

    train_val_spectrograms, test_spectrograms, train_val_labels, test_labels = train_test_split(processed_spectrograms, labels, test_size=0.2, random_state=42, stratify=labels)

    train_spectrograms, val_spectrograms, train_labels, val_labels = train_test_split(train_val_spectrograms, train_val_labels, test_size=0.125, random_state=42, stratify=train_val_labels)
    # 0.125 * 80% = 10%

    train_dataset = SpectrogramDataset(train_spectrograms, train_labels)
    val_dataset = SpectrogramDataset(val_spectrograms, val_labels)
    test_dataset = SpectrogramDataset(test_spectrograms, test_labels)

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    best_model = compare_models(train_loader, val_loader, labels)

    model_save_path = './best_model.pth'
    save_model_neural(best_model, model_save_path)
    print(f"Model saved to: {model_save_path}")

    f1_val = calculate_f1_score(best_model, val_loader, device)
    print(f"f1-score on validation_set (10%): {f1_val:.4f}")

    f1_test = calculate_f1_score(best_model, test_loader, device)
    print(f"f1-score on test_set (20%): {f1_test:.4f}")

    file_ids = [os.path.basename(file) for file in processed_npy_files] 


    print("Tracking most and least difficult samples on validation set:")
    track_difficult_samples(best_model, val_loader, device, file_ids)

    print("Confusion Matrix on Validation Set:")
    class_names = ['Class 0', 'Class 1']  
    plot_confusion_matrix(best_model, val_loader, device, class_names)


    #I'll add Random Forest just because i like it
    # best_rf_model, best_params, rf_f1_score = random_forest_with_random_search(spectrograms, labels, test_size=0.2, n_iter=10, cv=3)

    # save_model_classic(best_rf_model, 'best_random_forest_model.joblib')

